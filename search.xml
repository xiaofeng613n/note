<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flume配置]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F15%2Fflume%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7flume:1.7kafka:kafka_2.10-0.10.1.0(已安装好，kafka安装) 1.下载flume，解压1234# pwd/usr/local/soft/apache-flume-1.7.0-bin# lsbin CHANGELOG conf DEVNOTES doap_Flume.rdf docs lib LICENSE NOTICE README.md RELEASE-NOTES tools 2.配置在conf目录根据模板配置文件flume-conf.properties.template配置agent和collect这里在一台机器上配置了3个agent和1个collect,如下： agent1.properties123456789101112131415agent1.sources = s1agent1.channels = c1agent1.sinks = k1agent1.sources.s1.type=execagent1.sources.s1.command=tail -F /usr/local/soft/log/a.logagent1.sources.s1.channels=c1agent1.channels.c1.type=memoryagent1.channels.c1.capacity=10000agent1.channels.c1.transactionCapacity=100#设置接收器agent1.sinks.k1.channel=c1agent1.sinks.k1.type= avroagent1.sinks.k1.hostname=192.168.1.108agent1.sinks.k1.port=40041 agent2.properties123456789101112131415agent2.sources = s1agent2.channels = c1agent2.sinks = k1agent2.sources.s1.type=execagent2.sources.s1.command=tail -F /usr/local/soft/log/b.logagent2.sources.s1.channels=c1agent2.channels.c1.type=memoryagent2.channels.c1.capacity=10000agent2.channels.c1.transactionCapacity=100#设置接收器agent2.sinks.k1.channel=c1agent2.sinks.k1.type= avroagent2.sinks.k1.hostname=192.168.1.108agent2.sinks.k1.port=40041 agent3.properties123456789101112131415agent3.sources = s1agent3.channels = c1agent3.sinks = k1agent3.sources.s1.type=execagent3.sources.s1.command=tail -F /usr/local/soft/log/c.logagent3.sources.s1.channels=c1agent3.channels.c1.type=memoryagent3.channels.c1.capacity=10000agent3.channels.c1.transactionCapacity=100#设置接收器agent3.sinks.k1.channel=c1agent3.sinks.k1.type= avroagent3.sinks.k1.hostname=192.168.1.108agent3.sinks.k1.port=40041 collect.properties123456789101112131415161718192021222324agentx.sources = s1agentx.channels = c1agentx.sinks = k1agentx.sources.s1.channels = c1agentx.sources.s1.type = avroagentx.sources.s1.bind = 192.168.1.108agentx.sources.s1.port = 40041agentx.sources.s1.threads = 2agentx.channels.c1.type=memoryagentx.channels.c1.capacity=10000agentx.channels.c1.transactionCapacity=100#设置kafka接收器agentx.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka的broker地址和端口号agentx.sinks.k1.brokerList=192.168.1.108:9093#设置Kafka的Topicagentx.sinks.k1.topic=topicOfFlume#设置序列化方式agentx.sinks.k1.serializer.class=kafka.serializer.StringEncoderagentx.sinks.k1.channel=c1 可以看出3个agent的配置基本相同，就是sources监听文件不一样 3.启动（注意：-n 后的参数要和配置文件中的agent名相同）在flume目录下执行如下1234# bin/flume-ng agent --conf ./conf/ -f conf/collect.properties -Dflume.root.logger=DEBUG,console -n agentx# bin/flume-ng agent --conf ./conf/ -f conf/agent1.properties -Dflume.root.logger=DEBUG,console -n agent1# bin/flume-ng agent --conf ./conf/ -f conf/agent2.properties -Dflume.root.logger=DEBUG,console -n agent2# bin/flume-ng agent --conf ./conf/ -f conf/agent3.properties -Dflume.root.logger=DEBUG,console -n agent3 启动是否成功可以查看（这是在一台机器上部署的）12345# jps | grep App*26160 Application41320 Application26266 Application26205 Application 4.测试手动或脚本向监听的文件添加内容，kafka中会有数据写入,测试消费： 123456789101112131415161718192021222324252627282930public class KafkaTest&#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; testConsumer(); &#125; public static void testConsumer() &#123; Properties props = new Properties(); props.put("bootstrap.servers", "192.168.1.108:9093"); props.put("group.id", "groupC");// props.put("enable.auto.commit", "true");// props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "earliest"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("topicOfFlume")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(2)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(3)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
