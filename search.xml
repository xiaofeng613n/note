<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HBase配置]]></title>
    <url>%2Fnote.github.io%2F2018%2F01%2F14%2FHBase%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7java:1.8 SSH设置和密钥生成SSH设置需要在集群上执行不同的操作，如启动，停止和分布式守护shell操作。进行身份验证不同的Hadoop用户，需要一种用于Hadoop的用户提供的公钥/私钥对，并用不同的用户共享。以下的命令被用于生成使用SSH密钥值对。复制公钥从id_rsa.pub为authorized_keys，并提供所有者，读写权限到authorized_keys文件。 12345# ssh-keygen -t rsa# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# chmod 0600 ~/.ssh///验证# ssh localhost 伪分布式模式下载Hbase12# wget http://mirrors.sonic.net/apache/hbase/1.4.0/hbase-1.4.0-bin.tar.gz# tar -zxvf hbase-1.4.0-bin.tar.gz 配置 $HBASE_HOME/conf/hbase-env.sh12export JAVA_HOME=/usr/java/jdk1.8.0_144/export HBASE_MANAGES_ZK=true 配置 $HBASE_HOME/conf/hbase-site.xml（这里hbase.rootdir使用的是文件系统，非hdfs）123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:/usr/local/soft/hbase-1.4.0/h-data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.port&lt;/name&gt; &lt;value&gt;16201&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16301&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 启动与停止123456789# cd $HBASE_HOME/bin # sh start-hbase.sh# sh stop-hbase.sh// 启动后可以使用jps命令看到如下进程# jps27442 HQuorumPeer27668 Jps27509 HMaster27593 HRegionServer hbase的web页面（机器的域名是linu1）http://linux1:16010/master-status 注意windows环境下用java api 连接hbase可能碰到的问题： window配置host：ip linux1 linux配置host(是的，linux也要配置，应为我们启动的是伪分布式集群):ip linux1 winutils.exe]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume配置]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F15%2Fflume%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7flume:1.7kafka:kafka_2.10-0.10.1.0(已安装好，kafka安装) 1.下载flume，解压1234# pwd/usr/local/soft/apache-flume-1.7.0-bin# lsbin CHANGELOG conf DEVNOTES doap_Flume.rdf docs lib LICENSE NOTICE README.md RELEASE-NOTES tools 2.配置在conf目录根据模板配置文件flume-conf.properties.template配置agent和collect这里在一台机器上配置了3个agent和1个collect,如下： agent1.properties123456789101112131415agent1.sources = s1agent1.channels = c1agent1.sinks = k1agent1.sources.s1.type=execagent1.sources.s1.command=tail -F /usr/local/soft/log/a.logagent1.sources.s1.channels=c1agent1.channels.c1.type=memoryagent1.channels.c1.capacity=10000agent1.channels.c1.transactionCapacity=100#设置接收器agent1.sinks.k1.channel=c1agent1.sinks.k1.type= avroagent1.sinks.k1.hostname=192.168.1.108agent1.sinks.k1.port=40041 agent2.properties123456789101112131415agent2.sources = s1agent2.channels = c1agent2.sinks = k1agent2.sources.s1.type=execagent2.sources.s1.command=tail -F /usr/local/soft/log/b.logagent2.sources.s1.channels=c1agent2.channels.c1.type=memoryagent2.channels.c1.capacity=10000agent2.channels.c1.transactionCapacity=100#设置接收器agent2.sinks.k1.channel=c1agent2.sinks.k1.type= avroagent2.sinks.k1.hostname=192.168.1.108agent2.sinks.k1.port=40041 agent3.properties123456789101112131415agent3.sources = s1agent3.channels = c1agent3.sinks = k1agent3.sources.s1.type=execagent3.sources.s1.command=tail -F /usr/local/soft/log/c.logagent3.sources.s1.channels=c1agent3.channels.c1.type=memoryagent3.channels.c1.capacity=10000agent3.channels.c1.transactionCapacity=100#设置接收器agent3.sinks.k1.channel=c1agent3.sinks.k1.type= avroagent3.sinks.k1.hostname=192.168.1.108agent3.sinks.k1.port=40041 collect.properties123456789101112131415161718192021222324agentx.sources = s1agentx.channels = c1agentx.sinks = k1agentx.sources.s1.channels = c1agentx.sources.s1.type = avroagentx.sources.s1.bind = 192.168.1.108agentx.sources.s1.port = 40041agentx.sources.s1.threads = 2agentx.channels.c1.type=memoryagentx.channels.c1.capacity=10000agentx.channels.c1.transactionCapacity=100#设置kafka接收器agentx.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka的broker地址和端口号agentx.sinks.k1.brokerList=192.168.1.108:9093#设置Kafka的Topicagentx.sinks.k1.topic=topicOfFlume#设置序列化方式agentx.sinks.k1.serializer.class=kafka.serializer.StringEncoderagentx.sinks.k1.channel=c1 可以看出3个agent的配置基本相同，就是sources监听文件不一样 3.启动（注意：-n 后的参数要和配置文件中的agent名相同）在flume目录下执行如下1234# bin/flume-ng agent --conf ./conf/ -f conf/collect.properties -Dflume.root.logger=DEBUG,console -n agentx# bin/flume-ng agent --conf ./conf/ -f conf/agent1.properties -Dflume.root.logger=DEBUG,console -n agent1# bin/flume-ng agent --conf ./conf/ -f conf/agent2.properties -Dflume.root.logger=DEBUG,console -n agent2# bin/flume-ng agent --conf ./conf/ -f conf/agent3.properties -Dflume.root.logger=DEBUG,console -n agent3 启动是否成功可以查看（这是在一台机器上部署的）12345# jps | grep App*26160 Application41320 Application26266 Application26205 Application 4.测试手动或脚本向监听的文件添加内容，kafka中会有数据写入,测试消费： 123456789101112131415161718192021222324252627282930public class KafkaTest&#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; testConsumer(); &#125; public static void testConsumer() &#123; Properties props = new Properties(); props.put("bootstrap.servers", "192.168.1.108:9093"); props.put("group.id", "groupC");// props.put("enable.auto.commit", "true");// props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "earliest"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("topicOfFlume")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(2)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(3)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 使用问题本地默认端口被占用导致无法访问window:123netstat -aon|findstr &quot;4000&quot; // 得到占用端口进程的pidtasklist|findstr &quot;884&quot; //根据pid查看具体应用taskkill /f /t /im FoxitProtect.exe //结束应用 linux:netstat -anp | grep 端口号或者netstat-tunlp查找，然后kill即可]]></content>
  </entry>
</search>
