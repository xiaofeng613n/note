<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flume总结]]></title>
    <url>%2Fnote.github.io%2F2018%2F06%2F30%2Fflume%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[参考： flume官网 大步流星的博客-flume Flume-ng FileChannel原理解析 随笔分类 - Flume-NG Flume FileChannel优化（扩展）实践指南 Flume架构与源码分析 基于Flume的美团日志收集系统(一)架构和设计 美团对 flume 的扩展和改进（代码） 魅族大数据之流平台设计部署实践 问题一个channel，多个sink Proposal of Transactional Multiplex (fan out) Sink Multiple Sinks can connect to single Channel Flume-ng:multi sink one channel两种配置方式的对比 其他 记Flume-NG一些注意事项 Spooling Directory Source虽然做不到实时，但是也可以通过日志文件的切分，做到准实时。 模块命名规则：所有的 Source 以 src 开头，所有的 Channel 以 ch 开头，所有的 Sink 以 sink 开头；]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume日志收集分层架构应用实践（转）]]></title>
    <url>%2Fnote.github.io%2F2018%2F06%2F30%2Fflume%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%EF%BC%88%E8%BD%AC%EF%BC%89%2F</url>
    <content type="text"><![CDATA[原文链接：Flume日志收集分层架构应用实践 为什么要对Flume日志收集系统进行分层设计基于Flume设计实现分层日志收集系统，到底有什么好处呢？我们可以先看一下，如果不分层，会带来哪些问题： 如果需要通过Kafka去缓冲上游基于Flume收集而构建的日志流，对于数据平台内部服务器产生的数据还好，但是如果日志数据是跨业务组，甚至是跨部门，那么就需要将Kafka相关信息暴露给外部，这样对Kafka的访问便不是数据平台内部可控的 如果是外部日志进入平台内部HDFS，这样如果需要对Hadoop系统进行升级或例行维护，这种直连的方式会影响到上游部署Flume的日志流的始端日志收集服务 如果数据平台内部某些系统，如Kafka集群、HDFS集群所在节点的机房位置变更，数据迁移，会使得依赖日志数据的外部系统受到不同程度的影响，外部系统需要相关开发或运维人员参与进来 由于收集日志的数据源端可能是外部一些服务器（多个单个的节点），一些业务集群（相互协作的多节点组），也可能是内部一些提供收集服务的服务节点，这些所有的服务器上部署的Flume Agent都处于一层中，比较难于分组管理由于所有数据源端Flume Agent收集的日志进入数据平台的时候，没有一个统一的类似总线的组件，很难因为某些业务扩展而独立地去升级数据平台内部的接收层服务节点，可能为了升级数据平台内部某个系统或服务而导致影响了其他的接收层服务节点 上图中，Flume日志收集系统采用两层架构设计：第一层（L1）是日志收集层，第二层（L2）是数据平台缓冲层（汇聚层）。通过这种方式，使得日志收集系统有如下特点： 针对数据平台外部的业务系统，根据需要分析的数据业务类型进行分组，属于同一种类型的业务日志，在数据平台前端增加了一个Flume汇聚层节点组，该组节点只影响到它对应的L1层的业务数据 如果Hadoop集群、Kafka需要停机维护或升级，对外部L1层Flume Agent没有影响，只需要在L2层做好数据的接收与缓冲即可，待维护或升级结束，继续将L2层缓存的数据导入到数据存储系统 如果外部某个类型的业务日志数据节点需要扩容，直接在L1层将数据流指向数据平台内部与之相对应的L2层Flume Agent节点组即可，能够对外部因业务变化发生的新增日志收集需求，进行快速地响应和部署 对于数据平台内部，因为收集日志的节点非常可控，可以直接通过L1层Flume Agent使日志数据流入HDFS或Kafka，当然为了架构统一和管理，最好也是通过L2层Flume Agent节点组来汇聚/缓冲L1层Flume Agent收集的日志数据 通过上面分析可见，分层无非是为了使的日志数据源节点的Flume Agent服务与数据平台的存储系统（Kafka/HDFS）进行解耦，同时能够更好地对同类型业务多节点的日志流进行一个聚合操作，并分离开独立管理。另外，可以根据实际业务需要，适当增加Flume系统分层，满足日志流数据的汇聚需要。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume源码-组件TaildirSource]]></title>
    <url>%2Fnote.github.io%2F2018%2F04%2F08%2Fflume%E6%BA%90%E7%A0%81-%E7%BB%84%E4%BB%B6TaildirSource%2F</url>
    <content type="text"><![CDATA[TaildirSource配置加载（configure方法）和资源关闭（stop方法）省略，主要是start和process方法： 12345678910111213public synchronized void start() &#123; ... reader = new ReliableTaildirEventReader.Builder() ... .build(); ... idleFileChecker.scheduleWithFixedDelay(new idleFileCheckerRunnable(),idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS); ... positionWriter.scheduleWithFixedDelay(new PositionWriterRunnable(),writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS); super.start(); sourceCounter.start(); &#125; ReliableTaildirEventReader初始化； idleFileChecker定时执行idleFileCheckerRunnable:找到idleTimeout时间内没有更新过的历史文件； positionWriter定时执行PositionWriterRunnable：existingInodes对象json格式化，写入文件 1234567891011121314151617181920212223242526ReliableTaildirEventReader:public Status process() &#123; Status status = Status.READY; ... existingInodes.clear(); existingInodes.addAll(reader.updateTailFiles()); for (long inode : existingInodes) &#123; TailFile tf = reader.getTailFiles().get(inode); if (tf.needTail()) &#123; tailFileProcess(tf, true); &#125; &#125; ... closeTailFiles(); TimeUnit.MILLISECONDS.sleep(retryInterval); ... return status;&#125; --&gt;ReliableTaildirEventReader.tailFileProcess--&gt;ReliableTaildirEventReader.readEvents--&gt;TailFile.readEvents--&gt;TailFile.readEvent--&gt;TailFile.readLine existingInodes.addAll(reader.updateTailFiles())这里调用reader.updateTailFiles方法遍历目录、遍历文件，找到匹配的有更新的文件，tailFiles缓存TailFile，返回inode list；该过程找到文件后，通过openFile(f, headers, inode, tf.getPos())方法包装成TailFile对象或者更新已有TailFile对象的信息updatePos(tf.getPath(), inode, 0)；历史文件是否更新根据lastUpdated记录时间判断；文件目录遍历的是List taildirCache，该对象在ReliableTaildirEventReader创建的时候加载的， tailFileProcess(TailFile tf, boolean backoffWithoutNL)循环变量文件，按行批量读取日志；TailFile内部有byte数组缓存数据；主要的文件读取逻辑都在这一步里面，我能大概看懂，具体细节还要自己看代码 closeTailFiles()从ReliableTaildirEventReader中找到idleInodes对应的TailFile执行关闭操作，这里的关闭操作会调用tailFileProcess，批量去读当前文件的行日志数据，然后关闭对应的RadomAccessFile 实现PollableSource接口：在process方法中刷新existingInodes，根据目录时间判断定时更新 idleInodes：通过文件更新时间判断；定时刷新position文件：找打existingInodes 对应的TailFile；]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume源码-组件]]></title>
    <url>%2Fnote.github.io%2F2018%2F03%2F25%2Fflume%E6%BA%90%E7%A0%81-%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Source&amp;SourceRunner&amp;ChannelProcessor&amp;ChannelSelectorflume的source分为两种12public interface PollableSource extends Source //有process()方法来处理。系统自动调用process()public interface EventDrivenSource extends Source //没有process()方法，而是通过start()来执行向channel中发送送数据的操作。 前面的flume启动过程已经分析过了，在调用AbstractConfigurationProvider.loadSources方法中，会根据配置的Source具体实现接口调用SourceRunner.forSource方法包装成对应的SourceRunner 123456789101112131415public static SourceRunner forSource(Source source) &#123; SourceRunner runner = null; if (source instanceof PollableSource) &#123; runner = new PollableSourceRunner(); ((PollableSourceRunner) runner).setSource((PollableSource) source); &#125; else if (source instanceof EventDrivenSource) &#123; runner = new EventDrivenSourceRunner(); ((EventDrivenSourceRunner) runner).setSource((EventDrivenSource) source); &#125; else &#123; throw new IllegalArgumentException(&quot;No known runner type for source &quot;+ source); &#125; return runner; &#125; 12345678910111213public class EventDrivenSourceRunner extends SourceRunner &#123; private LifecycleState lifecycleState; ... @Override public void start() &#123; Source source = getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); source.start(); lifecycleState = LifecycleState.START; &#125; ...&#125; EventDrivenSourceRunner的start方法逻辑比较简单： 初始化ChannelProcessor 执行source.start()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class PollableSourceRunner extends SourceRunner &#123; private static final Logger logger = LoggerFactory.getLogger(PollableSourceRunner.class); private AtomicBoolean shouldStop; private CounterGroup counterGroup; private PollingRunner runner; private Thread runnerThread; private LifecycleState lifecycleState; @Override public void start() &#123; PollableSource source = (PollableSource) getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); source.start(); runner = new PollingRunner(); runner.source = source; runner.counterGroup = counterGroup; runner.shouldStop = shouldStop; runnerThread = new Thread(runner); runnerThread.setName(getClass().getSimpleName() + &quot;-&quot; + source.getClass().getSimpleName() + &quot;-&quot; + source.getName()); runnerThread.start(); lifecycleState = LifecycleState.START; &#125; public static class PollingRunner implements Runnable &#123; private PollableSource source; ... @Override public void run() &#123; ... while (!shouldStop.get()) &#123; counterGroup.incrementAndGet(&quot;runner.polls&quot;); ... if (source.process().equals(PollableSource.Status.BACKOFF)) &#123; //调用process方法 counterGroup.incrementAndGet(&quot;runner.backoffs&quot;); Thread.sleep(Math.min( counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;) * source.getBackOffSleepIncrement(), source.getMaxBackOffSleepInterval())); &#125; else &#123; counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L); &#125; ... &#125; &#125; &#125;&#125; PollableSourceRunner的start 初始化ChannelProcessor 启动一个线程runnerThread，循环调用source（PollableSource）的process方法（如果失败等待超时时间之后重试） 123ChannelProcessor MultiplexingChannelSelector // 可以选择该发往哪些channel ReplicatingChannelSelector // allows the event to be placed in all the channels Sink&amp;SinkRunner&amp;SinkProcessor&amp;SinkSelectorflume启动分析过了SinkRunner可能对应一个sink也可能对应一个sinkgroup。因为如果配置文件中有sinkgroup则这个sinkgroup对应的sink会组成一个group然后封装为一个sinkRunner，然后不在sinkgroup中的sink会自己成为一个sinkRunner 123SinkRunner SinkProcessor sinks 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class SinkRunner implements LifecycleAware &#123; ... private CounterGroup counterGroup; private PollingRunner runner; private Thread runnerThread; private LifecycleState lifecycleState; private SinkProcessor policy; ... @Override public void start() &#123; SinkProcessor policy = getPolicy(); policy.start(); runner = new PollingRunner(); runner.policy = policy; runner.counterGroup = counterGroup; runner.shouldStop = new AtomicBoolean(); runnerThread = new Thread(runner); runnerThread.setName(&quot;SinkRunner-PollingRunner-&quot; + policy.getClass().getSimpleName()); runnerThread.start(); lifecycleState = LifecycleState.START; &#125; ... public static class PollingRunner implements Runnable &#123; ... private SinkProcessor policy; private AtomicBoolean shouldStop; private CounterGroup counterGroup; ... @Override public void run() &#123; ... while (!shouldStop.get()) &#123; if (policy.process().equals(Sink.Status.BACKOFF)) &#123; counterGroup.incrementAndGet(&quot;runner.backoffs&quot;); Thread.sleep(Math.min( counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;) * backoffSleepIncrement, maxBackoffSleep)); &#125; else &#123; counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L); &#125; &#125; ... &#125; &#125;&#125; SinkRunner的start方法逻辑和PollableSourceRunner.start类似： 调用SinkProcessor的start方法 启动一个线程runnerThread去轮询SinkProcessor的process方法 1234567891011121314151617public class DefaultSinkProcessor implements SinkProcessor, ConfigurableComponent &#123; private Sink sink; private LifecycleState lifecycleState; @Override public void start() &#123; Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set"); sink.start(); lifecycleState = LifecycleState.START; &#125; ... @Override public Status process() throws EventDeliveryException &#123; return sink.process(); &#125; ...&#125; 没有sinkgroup的情况对应DefaultSinkProcessor，DefaultSinkProcessor的process直接调用sink.process12345DefaultSinkProcessorLoadBalancingSinkProcessor RoundRobinSinkSelector RandomOrderSinkSelectorFailoverSinkProcessor Channelchannel的启动也是调用start方法，具体channel的事务实现单独分析…]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume源码-启动]]></title>
    <url>%2Fnote.github.io%2F2018%2F03%2F23%2Fflume%E6%BA%90%E7%A0%81-%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[flume的启动是从Application.main开始的，这里先忽略flume配置文件的加载读取（假设已经读取配置），从最简单的启动方式来看一下flume的启动流程 Application.java 31行application.handleConfigurationEvent(configurationProvider.getConfiguration()); AbstractConfigurationProvider：1234567public MaterializedConfiguration getConfiguration() &#123; ... loadChannels(agentConf, channelComponentMap); loadSources(agentConf, channelComponentMap, sourceRunnerMap); loadSinks(agentConf, channelComponentMap, sinkRunnerMap); ...&#125; getConfiguration这个方法的主要功能是物化配置 接下来分别分析下这三个加载方法： 123456private void loadChannels(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap) throws InstantiationException&#123; ... Channel channel = getOrCreateChannel(...); channelComponentMap.put(chName, new ChannelComponent(channel)); ...&#125; channel生成后，使用了一个ChannelComponent对象来包装它123456789private void loadSources(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap, Map&lt;String, SourceRunner&gt; sourceRunnerMap) throws InstantiationException &#123; ... Source source = sourceFactory.create(...) ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig); ChannelProcessor channelProcessor = new ChannelProcessor(selector); source.setChannelProcessor(channelProcessor); sourceRunnerMap.put(sourceName, SourceRunner.forSource(source)); ...&#125; SourceRunner结构：123456SourceRunner Source ChannelProcessor ChannelSelector channels InterceptorChain 12345678910111213141516171819202122private void loadSinks(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap, Map&lt;String, SinkRunner&gt; sinkRunnerMap) throws InstantiationException &#123; ... Sink sink = sinkFactory.create(...) sink.setChannel(channelComponent.channel); sinks.put(sinkName, sink); loadSinkGroups(agentConf, sinks, sinkRunnerMap);&#125;private void loadSinkGroups(AgentConfiguration agentConf, Map&lt;String, Sink&gt; sinks, Map&lt;String, SinkRunner&gt; sinkRunnerMap) throws InstantiationException &#123; ... SinkGroup group = new SinkGroup(groupSinks); sinkRunnerMap.put(comp.getComponentName(),new SinkRunner(group.getProcessor())); ... ... SinkProcessor pr = new DefaultSinkProcessor(); List&lt;Sink&gt; sinkMap = new ArrayList&lt;Sink&gt;(); sinkMap.add(entry.getValue()); pr.setSinks(sinkMap); Configurables.configure(pr, new Context()); sinkRunnerMap.put(entry.getKey(), new SinkRunner(pr)); ...&#125; SinkRunner结构： 1234567SinkGroup sinks SinkProcessorSinkRunner SinkProcessor SinkSelector sinks SinkRunner可能对应一个sink也可能对应一个sinkgroup。因为如果配置文件中有sinkgroup则这个sinkgroup对应的sink会组成一个group然后封装为一个sinkRunner，然后不在sinkgroup中的sink会自己成为一个sinkRunner 物化配置完成后，启动容器1234567891011121314151617181920212223242526272829public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) &#123; stopAllComponents(); startAllComponents(conf);&#125;private void startAllComponents(MaterializedConfiguration materializedConfiguration) &#123; ... for (Entry&lt;String, Channel&gt; entry : materializedConfiguration.getChannels().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; /* * Wait for all channels to start. */ ... for (Entry&lt;String, SinkRunner&gt; entry : materializedConfiguration.getSinkRunners().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; for (Entry&lt;String, SourceRunner&gt; entry : materializedConfiguration.getSourceRunners().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; ... &#125; stopAllComponents()方法会依次stop各个组件的运行，顺序是：source、sink、channel。之所以有顺序是因为：一、source是不停的读数据放入channel的；二、sink是不停的从channel拿数据的，channel两头都在使用应该最后停止，停止向channel发送数据后sink停止才不会丢数据。stop是通过supervisor.unsupervise方法来完成的。startAllComponents(conf)是启动各个组件的，顺序正好和stopAllComponents()停止顺序相反在startAllComponents中可以看到3个主要的循环都执行supervisor.supervise(…)这是在向LifecycleSupervisor中注册Supervisee（LifecycleAware ）启动channel组件后需要等待一定时间，是为了让所有channel全部启动 12345678910111213141516171819public synchronized void supervise(LifecycleAware lifecycleAware, SupervisorPolicy policy, LifecycleState desiredState) &#123; ... Supervisoree process = new Supervisoree(); process.status = new Status(); process.policy = policy; process.status.desiredState = desiredState; process.status.error = false; MonitorRunnable monitorRunnable = new MonitorRunnable(); monitorRunnable.lifecycleAware = lifecycleAware; monitorRunnable.supervisoree = process; monitorRunnable.monitorService = monitorService; supervisedProcesses.put(lifecycleAware, process); ScheduledFuture&lt;?&gt; future = monitorService.scheduleWithFixedDelay(monitorRunnable, 0, 3, TimeUnit.SECONDS); monitorFutures.put(lifecycleAware, future); &#125; monitorService定时执行MonitorRunnable，这个Runnable实现主要的逻辑就是，如果lifecycleAware的状态（supervisoree.status.desiredState）不是期望的状态就执行lifecycleAware的start或者stop方法 （MonitorRunnable保存的lifecycleAware就是前面加载的Channel，SourceRunner,SinkRunner）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase配置]]></title>
    <url>%2Fnote.github.io%2F2018%2F01%2F14%2FHBase%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7java:1.8 SSH设置和密钥生成SSH设置需要在集群上执行不同的操作，如启动，停止和分布式守护shell操作。进行身份验证不同的Hadoop用户，需要一种用于Hadoop的用户提供的公钥/私钥对，并用不同的用户共享。以下的命令被用于生成使用SSH密钥值对。复制公钥从id_rsa.pub为authorized_keys，并提供所有者，读写权限到authorized_keys文件。 12345# ssh-keygen -t rsa# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# chmod 0600 ~/.ssh///验证# ssh localhost 伪分布式模式下载Hbase12# wget http://mirrors.sonic.net/apache/hbase/1.4.0/hbase-1.4.0-bin.tar.gz# tar -zxvf hbase-1.4.0-bin.tar.gz 配置 $HBASE_HOME/conf/hbase-env.sh12export JAVA_HOME=/usr/java/jdk1.8.0_144/export HBASE_MANAGES_ZK=true 配置 $HBASE_HOME/conf/hbase-site.xml（这里hbase.rootdir使用的是文件系统，非hdfs）123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:/usr/local/soft/hbase-1.4.0/h-data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.port&lt;/name&gt; &lt;value&gt;16201&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16301&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 启动与停止123456789# cd $HBASE_HOME/bin # sh start-hbase.sh# sh stop-hbase.sh// 启动后可以使用jps命令看到如下进程# jps27442 HQuorumPeer27668 Jps27509 HMaster27593 HRegionServer hbase的web页面（机器的域名是linu1）http://linux1:16010/master-status 注意windows环境下用java api 连接hbase可能碰到的问题： window配置host：ip linux1 linux配置host(是的，linux也要配置，应为我们启动的是伪分布式集群):ip linux1 winutils.exe]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume安装与配置]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F15%2Fflume%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7flume:1.7kafka:kafka_2.10-0.10.1.0(已安装好，kafka安装) 1.下载flume，解压1234# pwd/usr/local/soft/apache-flume-1.7.0-bin# lsbin CHANGELOG conf DEVNOTES doap_Flume.rdf docs lib LICENSE NOTICE README.md RELEASE-NOTES tools 2.配置在conf目录根据模板配置文件flume-conf.properties.template配置agent和collect这里在一台机器上配置了3个agent和1个collect,如下： agent1.properties123456789101112131415agent1.sources = s1agent1.channels = c1agent1.sinks = k1agent1.sources.s1.type=execagent1.sources.s1.command=tail -F /usr/local/soft/log/a.logagent1.sources.s1.channels=c1agent1.channels.c1.type=memoryagent1.channels.c1.capacity=10000agent1.channels.c1.transactionCapacity=100#设置接收器agent1.sinks.k1.channel=c1agent1.sinks.k1.type= avroagent1.sinks.k1.hostname=192.168.1.108agent1.sinks.k1.port=40041 agent2.properties123456789101112131415agent2.sources = s1agent2.channels = c1agent2.sinks = k1agent2.sources.s1.type=execagent2.sources.s1.command=tail -F /usr/local/soft/log/b.logagent2.sources.s1.channels=c1agent2.channels.c1.type=memoryagent2.channels.c1.capacity=10000agent2.channels.c1.transactionCapacity=100#设置接收器agent2.sinks.k1.channel=c1agent2.sinks.k1.type= avroagent2.sinks.k1.hostname=192.168.1.108agent2.sinks.k1.port=40041 agent3.properties123456789101112131415agent3.sources = s1agent3.channels = c1agent3.sinks = k1agent3.sources.s1.type=execagent3.sources.s1.command=tail -F /usr/local/soft/log/c.logagent3.sources.s1.channels=c1agent3.channels.c1.type=memoryagent3.channels.c1.capacity=10000agent3.channels.c1.transactionCapacity=100#设置接收器agent3.sinks.k1.channel=c1agent3.sinks.k1.type= avroagent3.sinks.k1.hostname=192.168.1.108agent3.sinks.k1.port=40041 collect.properties123456789101112131415161718192021222324agentx.sources = s1agentx.channels = c1agentx.sinks = k1agentx.sources.s1.channels = c1agentx.sources.s1.type = avroagentx.sources.s1.bind = 192.168.1.108agentx.sources.s1.port = 40041agentx.sources.s1.threads = 2agentx.channels.c1.type=memoryagentx.channels.c1.capacity=10000agentx.channels.c1.transactionCapacity=100#设置kafka接收器agentx.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka的broker地址和端口号agentx.sinks.k1.brokerList=192.168.1.108:9093#设置Kafka的Topicagentx.sinks.k1.topic=topicOfFlume#设置序列化方式agentx.sinks.k1.serializer.class=kafka.serializer.StringEncoderagentx.sinks.k1.channel=c1 可以看出3个agent的配置基本相同，就是sources监听文件不一样 3.启动（注意：-n 后的参数要和配置文件中的agent名相同）在flume目录下执行如下1234# bin/flume-ng agent --conf ./conf/ -f conf/collect.properties -Dflume.root.logger=DEBUG,console -n agentx# bin/flume-ng agent --conf ./conf/ -f conf/agent1.properties -Dflume.root.logger=DEBUG,console -n agent1# bin/flume-ng agent --conf ./conf/ -f conf/agent2.properties -Dflume.root.logger=DEBUG,console -n agent2# bin/flume-ng agent --conf ./conf/ -f conf/agent3.properties -Dflume.root.logger=DEBUG,console -n agent3 启动是否成功可以查看（这是在一台机器上部署的）12345# jps | grep App*26160 Application41320 Application26266 Application26205 Application 4.测试手动或脚本向监听的文件添加内容，kafka中会有数据写入,测试消费： 123456789101112131415161718192021222324public class KafkaTest&#123; public static void main(String[] args) throws ExecutionException, InterruptedException&#123; testConsumer(); &#125; public static void testConsumer()&#123; Properties props = new Properties(); props.put("bootstrap.servers", "192.168.1.108:9093"); props.put("group.id", "groupC");// props.put("enable.auto.commit", "true");// props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "earliest"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("topicOfFlume")); while (true)&#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records)&#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post2%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 使用问题本地默认端口被占用导致无法访问window:123netstat -aon|findstr &quot;4000&quot; // 得到占用端口进程的pidtasklist|findstr &quot;884&quot; //根据pid查看具体应用taskkill /f /t /im FoxitProtect.exe //结束应用 linux:netstat -anp | grep 端口号或者netstat-tunlp查找，然后kill即可]]></content>
  </entry>
</search>
