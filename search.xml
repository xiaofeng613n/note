<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[flume源码-组件]]></title>
    <url>%2Fnote.github.io%2F2018%2F03%2F25%2Fflume%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Source&amp;SourceRunner&amp;ChannelProcessor&amp;ChannelSelectorflume的source分为两种12public interface PollableSource extends Source //有process()方法来处理。系统自动调用process()public interface EventDrivenSource extends Source //没有process()方法，而是通过start()来执行向channel中发送送数据的操作。 前面的flume启动过程已经分析过了，在调用AbstractConfigurationProvider.loadSources方法中，会根据配置的Source具体实现接口调用SourceRunner.forSource方法包装成对应的SourceRunner 123456789101112131415public static SourceRunner forSource(Source source) &#123; SourceRunner runner = null; if (source instanceof PollableSource) &#123; runner = new PollableSourceRunner(); ((PollableSourceRunner) runner).setSource((PollableSource) source); &#125; else if (source instanceof EventDrivenSource) &#123; runner = new EventDrivenSourceRunner(); ((EventDrivenSourceRunner) runner).setSource((EventDrivenSource) source); &#125; else &#123; throw new IllegalArgumentException(&quot;No known runner type for source &quot;+ source); &#125; return runner; &#125; 12345678910111213public class EventDrivenSourceRunner extends SourceRunner &#123; private LifecycleState lifecycleState; ... @Override public void start() &#123; Source source = getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); source.start(); lifecycleState = LifecycleState.START; &#125; ...&#125; EventDrivenSourceRunner的start方法逻辑比较简单： 初始化ChannelProcessor 执行source.start()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class PollableSourceRunner extends SourceRunner &#123; private static final Logger logger = LoggerFactory.getLogger(PollableSourceRunner.class); private AtomicBoolean shouldStop; private CounterGroup counterGroup; private PollingRunner runner; private Thread runnerThread; private LifecycleState lifecycleState; @Override public void start() &#123; PollableSource source = (PollableSource) getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); source.start(); runner = new PollingRunner(); runner.source = source; runner.counterGroup = counterGroup; runner.shouldStop = shouldStop; runnerThread = new Thread(runner); runnerThread.setName(getClass().getSimpleName() + &quot;-&quot; + source.getClass().getSimpleName() + &quot;-&quot; + source.getName()); runnerThread.start(); lifecycleState = LifecycleState.START; &#125; public static class PollingRunner implements Runnable &#123; private PollableSource source; ... @Override public void run() &#123; ... while (!shouldStop.get()) &#123; counterGroup.incrementAndGet(&quot;runner.polls&quot;); ... if (source.process().equals(PollableSource.Status.BACKOFF)) &#123; //调用process方法 counterGroup.incrementAndGet(&quot;runner.backoffs&quot;); Thread.sleep(Math.min( counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;) * source.getBackOffSleepIncrement(), source.getMaxBackOffSleepInterval())); &#125; else &#123; counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L); &#125; ... &#125; &#125; &#125;&#125; PollableSourceRunner的start 初始化ChannelProcessor 启动一个线程runnerThread，循环调用source（PollableSource）的process方法（如果失败等待超时时间之后重试） 123ChannelProcessor MultiplexingChannelSelector // 可以选择该发往哪些channel ReplicatingChannelSelector // allows the event to be placed in all the channels Sink&amp;SinkRunner&amp;SinkProcessor&amp;SinkSelectorflume启动分析过了SinkRunner可能对应一个sink也可能对应一个sinkgroup。因为如果配置文件中有sinkgroup则这个sinkgroup对应的sink会组成一个group然后封装为一个sinkRunner，然后不在sinkgroup中的sink会自己成为一个sinkRunner 123SinkRunner SinkProcessor sinks 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class SinkRunner implements LifecycleAware &#123; ... private CounterGroup counterGroup; private PollingRunner runner; private Thread runnerThread; private LifecycleState lifecycleState; private SinkProcessor policy; ... @Override public void start() &#123; SinkProcessor policy = getPolicy(); policy.start(); runner = new PollingRunner(); runner.policy = policy; runner.counterGroup = counterGroup; runner.shouldStop = new AtomicBoolean(); runnerThread = new Thread(runner); runnerThread.setName(&quot;SinkRunner-PollingRunner-&quot; + policy.getClass().getSimpleName()); runnerThread.start(); lifecycleState = LifecycleState.START; &#125; ... public static class PollingRunner implements Runnable &#123; ... private SinkProcessor policy; private AtomicBoolean shouldStop; private CounterGroup counterGroup; ... @Override public void run() &#123; ... while (!shouldStop.get()) &#123; if (policy.process().equals(Sink.Status.BACKOFF)) &#123; counterGroup.incrementAndGet(&quot;runner.backoffs&quot;); Thread.sleep(Math.min( counterGroup.incrementAndGet(&quot;runner.backoffs.consecutive&quot;) * backoffSleepIncrement, maxBackoffSleep)); &#125; else &#123; counterGroup.set(&quot;runner.backoffs.consecutive&quot;, 0L); &#125; &#125; ... &#125; &#125;&#125; SinkRunner的start方法逻辑和PollableSourceRunner.start类似： 调用SinkProcessor的start方法 启动一个线程runnerThread去轮询SinkProcessor的process方法 1234567891011121314151617public class DefaultSinkProcessor implements SinkProcessor, ConfigurableComponent &#123; private Sink sink; private LifecycleState lifecycleState; @Override public void start() &#123; Preconditions.checkNotNull(sink, "DefaultSinkProcessor sink not set"); sink.start(); lifecycleState = LifecycleState.START; &#125; ... @Override public Status process() throws EventDeliveryException &#123; return sink.process(); &#125; ...&#125; 没有sinkgroup的情况对应DefaultSinkProcessor，DefaultSinkProcessor的process直接调用sink.process12345DefaultSinkProcessorLoadBalancingSinkProcessor RoundRobinSinkSelector RandomOrderSinkSelectorFailoverSinkProcessor Channel…]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume源码-启动]]></title>
    <url>%2Fnote.github.io%2F2018%2F03%2F23%2Fflume%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[flume的启动是从Application.main开始的，这里先忽略flume配置文件的加载读取（假设已经读取配置），从最简单的启动方式来看一下flume的启动流程 Application.java 31行application.handleConfigurationEvent(configurationProvider.getConfiguration()); 12345678AbstractConfigurationProvider：public MaterializedConfiguration getConfiguration() &#123; ... loadChannels(agentConf, channelComponentMap); loadSources(agentConf, channelComponentMap, sourceRunnerMap); loadSinks(agentConf, channelComponentMap, sinkRunnerMap); ...&#125; getConfiguration这个方法的主要功能是物化配置 接下来分别分析下这三个加载方法： 1234567AbstractConfigurationProvider:private void loadChannels(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap) throws InstantiationException&#123; ... Channel channel = getOrCreateChannel(...); channelComponentMap.put(chName, new ChannelComponent(channel)); ...&#125; channel生成后，使用了一个ChannelComponent对象来包装它12345678910AbstractConfigurationProvider:private void loadSources(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap, Map&lt;String, SourceRunner&gt; sourceRunnerMap) throws InstantiationException &#123; ... Source source = sourceFactory.create(...) ChannelSelector selector = ChannelSelectorFactory.create(sourceChannels, selectorConfig); ChannelProcessor channelProcessor = new ChannelProcessor(selector); source.setChannelProcessor(channelProcessor); sourceRunnerMap.put(sourceName, SourceRunner.forSource(source)); ...&#125; SourceRunner结构：123456SourceRunner Source ChannelProcessor ChannelSelector channels InterceptorChain 1234567891011121314151617181920212223AbstractConfigurationProvider:private void loadSinks(AgentConfiguration agentConf, Map&lt;String, ChannelComponent&gt; channelComponentMap, Map&lt;String, SinkRunner&gt; sinkRunnerMap) throws InstantiationException &#123; ... Sink sink = sinkFactory.create(...) sink.setChannel(channelComponent.channel); sinks.put(sinkName, sink); loadSinkGroups(agentConf, sinks, sinkRunnerMap);&#125;private void loadSinkGroups(AgentConfiguration agentConf, Map&lt;String, Sink&gt; sinks, Map&lt;String, SinkRunner&gt; sinkRunnerMap) throws InstantiationException &#123; ... SinkGroup group = new SinkGroup(groupSinks); sinkRunnerMap.put(comp.getComponentName(),new SinkRunner(group.getProcessor())); ... ... SinkProcessor pr = new DefaultSinkProcessor(); List&lt;Sink&gt; sinkMap = new ArrayList&lt;Sink&gt;(); sinkMap.add(entry.getValue()); pr.setSinks(sinkMap); Configurables.configure(pr, new Context()); sinkRunnerMap.put(entry.getKey(), new SinkRunner(pr)); ...&#125; SinkRunner结构： 1234567SinkGroup sinks SinkProcessorSinkRunner SinkProcessor SinkSelector sinks SinkRunner可能对应一个sink也可能对应一个sinkgroup。因为如果配置文件中有sinkgroup则这个sinkgroup对应的sink会组成一个group然后封装为一个sinkRunner，然后不在sinkgroup中的sink会自己成为一个sinkRunner 物化配置完成后，启动容器1234567891011121314151617181920212223242526272829public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) &#123; stopAllComponents(); startAllComponents(conf);&#125;private void startAllComponents(MaterializedConfiguration materializedConfiguration) &#123; ... for (Entry&lt;String, Channel&gt; entry : materializedConfiguration.getChannels().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; /* * Wait for all channels to start. */ ... for (Entry&lt;String, SinkRunner&gt; entry : materializedConfiguration.getSinkRunners().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; for (Entry&lt;String, SourceRunner&gt; entry : materializedConfiguration.getSourceRunners().entrySet()) &#123; ... supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); ... &#125; ... &#125; stopAllComponents()方法会依次stop各个组件的运行，顺序是：source、sink、channel。之所以有顺序是因为：一、source是不停的读数据放入channel的；二、sink是不停的从channel拿数据的，channel两头都在使用应该最后停止，停止向channel发送数据后sink停止才不会丢数据。stop是通过supervisor.unsupervise方法来完成的。startAllComponents(conf)是启动各个组件的，顺序正好和stopAllComponents()停止顺序相反在startAllComponents中可以看到3个主要的循环都执行supervisor.supervise(…)这是在向LifecycleSupervisor中注册Supervisee（LifecycleAware ）启动channel组件后需要等待一定时间，是为了让所有channel全部启动 12345678910111213141516171819public synchronized void supervise(LifecycleAware lifecycleAware, SupervisorPolicy policy, LifecycleState desiredState) &#123; ... Supervisoree process = new Supervisoree(); process.status = new Status(); process.policy = policy; process.status.desiredState = desiredState; process.status.error = false; MonitorRunnable monitorRunnable = new MonitorRunnable(); monitorRunnable.lifecycleAware = lifecycleAware; monitorRunnable.supervisoree = process; monitorRunnable.monitorService = monitorService; supervisedProcesses.put(lifecycleAware, process); ScheduledFuture&lt;?&gt; future = monitorService.scheduleWithFixedDelay(monitorRunnable, 0, 3, TimeUnit.SECONDS); monitorFutures.put(lifecycleAware, future); &#125; monitorService定时执行MonitorRunnable，这个Runnable实现主要的逻辑就是，如果lifecycleAware的状态（supervisoree.status.desiredState）不是期望的状态就执行lifecycleAware的start或者stop方法 （MonitorRunnable保存的lifecycleAware就是前面加载的Channel，SourceRunner,SinkRunner）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase配置]]></title>
    <url>%2Fnote.github.io%2F2018%2F01%2F14%2FHBase%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7java:1.8 SSH设置和密钥生成SSH设置需要在集群上执行不同的操作，如启动，停止和分布式守护shell操作。进行身份验证不同的Hadoop用户，需要一种用于Hadoop的用户提供的公钥/私钥对，并用不同的用户共享。以下的命令被用于生成使用SSH密钥值对。复制公钥从id_rsa.pub为authorized_keys，并提供所有者，读写权限到authorized_keys文件。 12345# ssh-keygen -t rsa# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# chmod 0600 ~/.ssh///验证# ssh localhost 伪分布式模式下载Hbase12# wget http://mirrors.sonic.net/apache/hbase/1.4.0/hbase-1.4.0-bin.tar.gz# tar -zxvf hbase-1.4.0-bin.tar.gz 配置 $HBASE_HOME/conf/hbase-env.sh12export JAVA_HOME=/usr/java/jdk1.8.0_144/export HBASE_MANAGES_ZK=true 配置 $HBASE_HOME/conf/hbase-site.xml（这里hbase.rootdir使用的是文件系统，非hdfs）123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:/usr/local/soft/hbase-1.4.0/h-data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.port&lt;/name&gt; &lt;value&gt;16201&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16301&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 启动与停止123456789# cd $HBASE_HOME/bin # sh start-hbase.sh# sh stop-hbase.sh// 启动后可以使用jps命令看到如下进程# jps27442 HQuorumPeer27668 Jps27509 HMaster27593 HRegionServer hbase的web页面（机器的域名是linu1）http://linux1:16010/master-status 注意windows环境下用java api 连接hbase可能碰到的问题： window配置host：ip linux1 linux配置host(是的，linux也要配置，应为我们启动的是伪分布式集群):ip linux1 winutils.exe]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume配置]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F15%2Fflume%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[环境linux:centos7flume:1.7kafka:kafka_2.10-0.10.1.0(已安装好，kafka安装) 1.下载flume，解压1234# pwd/usr/local/soft/apache-flume-1.7.0-bin# lsbin CHANGELOG conf DEVNOTES doap_Flume.rdf docs lib LICENSE NOTICE README.md RELEASE-NOTES tools 2.配置在conf目录根据模板配置文件flume-conf.properties.template配置agent和collect这里在一台机器上配置了3个agent和1个collect,如下： agent1.properties123456789101112131415agent1.sources = s1agent1.channels = c1agent1.sinks = k1agent1.sources.s1.type=execagent1.sources.s1.command=tail -F /usr/local/soft/log/a.logagent1.sources.s1.channels=c1agent1.channels.c1.type=memoryagent1.channels.c1.capacity=10000agent1.channels.c1.transactionCapacity=100#设置接收器agent1.sinks.k1.channel=c1agent1.sinks.k1.type= avroagent1.sinks.k1.hostname=192.168.1.108agent1.sinks.k1.port=40041 agent2.properties123456789101112131415agent2.sources = s1agent2.channels = c1agent2.sinks = k1agent2.sources.s1.type=execagent2.sources.s1.command=tail -F /usr/local/soft/log/b.logagent2.sources.s1.channels=c1agent2.channels.c1.type=memoryagent2.channels.c1.capacity=10000agent2.channels.c1.transactionCapacity=100#设置接收器agent2.sinks.k1.channel=c1agent2.sinks.k1.type= avroagent2.sinks.k1.hostname=192.168.1.108agent2.sinks.k1.port=40041 agent3.properties123456789101112131415agent3.sources = s1agent3.channels = c1agent3.sinks = k1agent3.sources.s1.type=execagent3.sources.s1.command=tail -F /usr/local/soft/log/c.logagent3.sources.s1.channels=c1agent3.channels.c1.type=memoryagent3.channels.c1.capacity=10000agent3.channels.c1.transactionCapacity=100#设置接收器agent3.sinks.k1.channel=c1agent3.sinks.k1.type= avroagent3.sinks.k1.hostname=192.168.1.108agent3.sinks.k1.port=40041 collect.properties123456789101112131415161718192021222324agentx.sources = s1agentx.channels = c1agentx.sinks = k1agentx.sources.s1.channels = c1agentx.sources.s1.type = avroagentx.sources.s1.bind = 192.168.1.108agentx.sources.s1.port = 40041agentx.sources.s1.threads = 2agentx.channels.c1.type=memoryagentx.channels.c1.capacity=10000agentx.channels.c1.transactionCapacity=100#设置kafka接收器agentx.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka的broker地址和端口号agentx.sinks.k1.brokerList=192.168.1.108:9093#设置Kafka的Topicagentx.sinks.k1.topic=topicOfFlume#设置序列化方式agentx.sinks.k1.serializer.class=kafka.serializer.StringEncoderagentx.sinks.k1.channel=c1 可以看出3个agent的配置基本相同，就是sources监听文件不一样 3.启动（注意：-n 后的参数要和配置文件中的agent名相同）在flume目录下执行如下1234# bin/flume-ng agent --conf ./conf/ -f conf/collect.properties -Dflume.root.logger=DEBUG,console -n agentx# bin/flume-ng agent --conf ./conf/ -f conf/agent1.properties -Dflume.root.logger=DEBUG,console -n agent1# bin/flume-ng agent --conf ./conf/ -f conf/agent2.properties -Dflume.root.logger=DEBUG,console -n agent2# bin/flume-ng agent --conf ./conf/ -f conf/agent3.properties -Dflume.root.logger=DEBUG,console -n agent3 启动是否成功可以查看（这是在一台机器上部署的）12345# jps | grep App*26160 Application41320 Application26266 Application26205 Application 4.测试手动或脚本向监听的文件添加内容，kafka中会有数据写入,测试消费： 123456789101112131415161718192021222324252627282930public class KafkaTest&#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; testConsumer(); &#125; public static void testConsumer() &#123; Properties props = new Properties(); props.put("bootstrap.servers", "192.168.1.108:9093"); props.put("group.id", "groupC");// props.put("enable.auto.commit", "true");// props.put("auto.commit.interval.ms", "1000"); props.put("auto.offset.reset", "earliest"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("topicOfFlume")); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.0.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>flume-ng</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(2)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%20(3)%2F</url>
    <content type="text"></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[my first post]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fmy-first-post%2F</url>
    <content type="text"></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>aop</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fnote.github.io%2F2017%2F10%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 使用问题本地默认端口被占用导致无法访问window:123netstat -aon|findstr &quot;4000&quot; // 得到占用端口进程的pidtasklist|findstr &quot;884&quot; //根据pid查看具体应用taskkill /f /t /im FoxitProtect.exe //结束应用 linux:netstat -anp | grep 端口号或者netstat-tunlp查找，然后kill即可]]></content>
  </entry>
</search>
